<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<!-- saved from url=(0043)http://auroraguorui.github.io/projects.html -->
<html xmlns="http://www.w3.org/1999/xhtml" class="gr__auroraguorui_github_io"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Projects</title>
    <link href="./Projects_files/style.css" rel="stylesheet" type="text/css" media="screen">
    <script type="text/javascript" async="" src="./Projects_files/ga.js"></script><script type="text/javascript" src="./Projects_files/jquery-1.6.min.js"></script>
	<script type="text/javascript" src="./Projects_files/jquery.reveal.js"></script>
    <meta name="keywords" content="Computer Science, Operating Systems, UIUC, Rui Guo">
    <script type="text/javascript">
    
    var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-3598266-1']);
      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    <script>
    function move_right(obj, value) {
    var x = document.getElementById('button_shadow' + obj.id);
    x.style.left = x.offsetLeft + value + 'px';}
    </script>
    
  </head>
  
  
  <body data-gr-c-s-loaded="true">
    <div id="wrapper">
	  <div id="logo">
		<h1><b>Rui Guo</b> <a href="https://github.com/auroraguorui?tab=repositories" target="_blank"><font size="2"><img src="./Projects_files/github.png" width="30," height="30/"></font></a><a href="https://www.linkedin.com/in/rui-guo-46031528" target="_blank"><font size="2"><img src="./Projects_files/LinkedIn.png" width="30," height="30" onmouseover="move_right(this, 5)"></font></a></h1>
	  </div>
	  <hr>
	  <!-- end #logo -->
	  <div id="header">
		<div id="menu">
		  <ul>
			<li><a href="http://auroraguorui.github.io/index.html"><b>Home</b></a></li>
			<li class="current_page_item"><a href="http://auroraguorui.github.io/projects.html#"><b>Projects</b></a></li>
			<li><a href="http://auroraguorui.github.io/course.html"><b>Course</b></a></li>
			<li><a href="http://auroraguorui.github.io/resume.html"><b>Resume</b></a></li>
			<li><a href="http://auroraguorui.github.io/publications.html"><b>Publications</b></a></li>
		  </ul>
		</div>
	  </div>
	  <!-- end #header -->
	  <!-- end #header-wrapper -->
	  <div id="page">
		<div id="content">
		  <div class="post">		
		    
			<h2 class="title">Web Application and Database</h2>
			<p style="margin-left:1.3em;">Tools: Python, Django, R, html5, MySQL, css3, jQuery, rpy2, Heroku, Scrapy, MongoDB, ggplot2, dplyr, maps, PyMongo, XPath Selectors</p>
			<p class="meta"></p>
			<div class="entry">
			
			  <div id="im">
			  <ul>
			  <li>
                  
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="HSK" data-animation="fade">
			  
			  <img src="./Projects_files/hsk.jpg" onmouseover="move_right(this, 5)"></a>
			  <h3>Web-based Online Testing for Skill Prediction</h3>
			  <ul>
			  			
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Establish a Computerized Chinese Language Proficiency Testing System [<a href="https://pacific-thicket-6533.herokuapp.com/Level4/register/" target="_blank">Demo</a>][<a href="https://github.com/auroraguorui/Django_web_application" target="_blank">code</a>]</p>
			  <p style="font-size:110%"><b>Method</b>: Web design, Database, Information theory, Bayesian estimation, Maximum likelihood estimation, Item response theory, EM algorithm, Expected a posterior</p>
			  <p style="font-size:110%"><b>Funded by</b>: <a href="http://www.chinesetest.cn/linkusm.do" target="_blank">Chinese Testing International Co., Ltd</a>
			  
              </p></ul>   
              <p>&nbsp;</p>

			  </li><li>
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="indeed" data-animation="fade">
			  <img src="./Projects_files/indeed.jpg" width="215" height="195" onmouseover="move_right(this, 5)"></a>
			  <h3>Indeed Web Crawling</h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Web Crawling Indeed.com to Analyze Data Scientist Job Postings [<a href="https://github.com/auroraguorui/Web-Scraping-and-API/tree/master/Indeed_Web_Scraping" target="_blank">code</a>]</p>
			  <p style="font-size:110%"><b>Method</b>: Web Crawling, Web Scraping, NoSQL database, MongoDB, Data visualization, Regular expressions</p>
			  
              </ul>
			  </li></ul></div>
			</div>
            <p>&nbsp;</p>

			<h2 class="title">Kaggle</h2>
			<p style="margin-left:1.3em;">Tools and Libraries: Python, R, Apache-Spark, PySpark, Pandas, Numpy, Scikit-Learn, Scipy, nltk, Cython, Tableau, gensim, BeautifulSoup, Regular expressions, ggplo2, maps, caret, dplyr</p>
			<p class="meta"></p>
			<div class="entry">
			  <div id="im">
			  <ul>
			  
			  <li>
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="Coupon" data-animation="fade">
			  <img src="./Projects_files/coupon.png" width="215" height="180"></a>
			  <h3><a href="https://www.kaggle.com/c/coupon-purchase-prediction" target="_blank">
			  Coupon Purchase Prediction</a> (Rank: top 5%)</h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Recommend a ranked list of coupons for each user and predict which coupons a customer will buy</p>
			  <p style="font-size:110%"><b>Method</b> : Content based collaborative filtering, Cross validation, Data visualization</p>
			  <p style="font-size:110%"><a href="https://github.com/auroraguorui/Kaggle_Competition/tree/master/Coupon_purchase_prediction" target="_blank">Source Code</a></p>
			  <p>&nbsp;</p>
			  
			  </ul>
			  
			  </li><li>
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="Springleaf" data-animation="fade">
			  <img src="./Projects_files/springleaf.dms" width="215" height="130"></a>
			  <h3><a href="https://www.kaggle.com/c/springleaf-marketing-response" target="_blank">
			  Springleaf Marketing Response</a> (Accuracy: 80%)</h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Determine whether to send a direct mail piece to a customer</p>
			  <p style="font-size:110%"><b>Method</b> : Amazon Web Service, Databricks, Data visualization, Spark SQLContext, Spark RDD, Spark mllib, High-dimensional Data</p>
			  <p style="font-size:110%"><a href="https://github.com/auroraguorui/Kaggle_Competition/tree/master/Springleaf" target="_blank">Source Code</a></p>
			  <p>&nbsp;</p>
			  
			  </ul>
                
              </li><li>
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="sentiment" data-animation="fade">
			  <img src="./Projects_files/sentiment.png" width="215" height="180"></a>
			  <h3><a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words" target="_blank">
			  Bag of Words, Bag of Popcorn</a> (Accuracy: 97.044%)</h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Sentiment Analysis on Movie Reviews</p>
			  <p style="font-size:110%"><b>Method</b>: Bag of words, TFIDF, naive bayes, 
			  Classification tree, Word2Vec, Doc2Vec, Adaboost, Logistic regression, 
			  Stochastic gradient descent, Word cloud, Ensembling</p>
			  <p style="font-size:110%"><a href="https://github.com/auroraguorui/Kaggle_Competition/tree/master/Bag_of_words" target="_blank">Source Code</a></p>
			  <p>&nbsp;</p>
			  
			  </ul>
			  
			  <!--	
			  <li>
			  <a href="#" class="big-link" data-reveal-id="SF" data-animation="fade" >
			  <img src="img/sf.png" width="215" height="180"></a>
			  <h3><a href="https://www.kaggle.com/c/sf-crime" target="_blank">
			  San Francisco Crime Classification</a></h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Predict the category of crimes that occurred in the city by the bay</p>
			  <p style="font-size:110%"><b>Method</b>: XGBoost, Random forests, K nearest neighbors, Naive bayes, Logistic regression, 
			     Cross validation, Data visualization</p>
			  <p></p>
			  <p style="font-size:110%"><a href="https://github.com/auroraguorui/Kaggle_Competition/blob/master/Digit_Recognition.py" target="_blank">Source Code</a></p>
			  <p>&nbsp;</p>
			  </li>
			  </ul>
			  
			  <li>
			  <a href="#" class="big-link" data-reveal-id="SF" data-animation="fade" >
			  <img src="img/sf.png" width="215" height="180"></a>
			  <h3><a href="https://www.kaggle.com/c/sf-crime" target="_blank">
			  Digit Recognition</a></h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Predict the category of crimes that occurred in the city by the bay</p>
			  <p style="font-size:110%"><b>Method</b>: XGBoost, Random forests, K nearest neighbors, Naive bayes, Logistic regression, 
			     Cross validation, Data visualization</p>
			  <p></p>
			  <p style="font-size:110%"><a href="https://github.com/auroraguorui/Kaggle_Competition/blob/master/Digit_Recognition.py" target="_blank">Source Code</a></p>
			  <p>&nbsp;</p>
			  </li>
			  </ul>
			  
                  
                  
			  
			  <li>
			  <a href="#" class="big-link" data-reveal-id="Titanic" data-animation="fade" >
			  <img src="img/titanic.jpeg"  width="215" height="180"></a>
			  <h3>Titanic Survival Prediction</h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Completed the analysis to predict which passengers were likely to survive the tragedy.</p>
              </ul>
			  <p style="font-size:110%"><b>Method</b>: Data manipulation, Neural networks, Support vector machines, XGBoost, Ensemble</p></ul>
			  <a href="https://github.com/auroraguorui/Kaggle_Competition/blob/master/survival_prediction.py" target="_blank">Source Code</a>
			  </li>
              -->
			  </li></ul></div>
			</div>
			
            
			<h2 class="title">Quantitative Research</h2>
			<p style="margin-left:1.3em;">Tools: SAS, R, Matlab</p>
			<p class="meta"></p>
			<div class="entry">
			  <div id="im">
			  <ul>
			  <li>                  
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="UI" data-animation="fade">
			  <img src="./Projects_files/uiuc.png" width="215" height="180"></a>
                  <h3>Mining Student Response Data for Knowledge</h3>
			  <ul>
			  <p></p>
			  <p style="font-size:110%"><b>Project</b>:  Pretest Item Selection Methods in Online Calibration for Cognitive Diagostic Model [<a href="http://auroraguorui.github.io/file/AERA2015_Rui_poster.pdf" target="_blank">poster</a>][<a href="https://github.com/auroraguorui/Quantitative-Research/tree/master/CDCAT_ONLINE" target="_blank">code</a>]</p>
			  <p style="font-size:110%"><b>Method</b>: Latent class model, Joint maximum likelihood,  Conditional maximum likelihood, Shannon entropy, Bayes theorem, Expectation-Maximization algorithm, Classification</p>
			  <p style="font-size:110%"><b>Funded by</b>: <a href="http://illinois.edu/" target="_blank">University of Illinois at Urbana Champaign</a></p>
			  <p style="font-size:110%">
			  
			  </p></ul>
			  <p>&nbsp;</p>
			  </li><li>
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="CB" data-animation="fade">
			  <img src="./Projects_files/collegeboard.jpg"></a>
			  <h3>College Board Research Fellowship</h3>
			  <ul>
			  <p></p>	
			  <p style="font-size:110%"><b>Project</b>: Data Analysis and Hypothesis Testing for ACCUPLACER<sup>®</sup> [<a href="http://auroraguorui.github.io/file/College_board.pdf" target="_blank">paper</a>][<a href="https://github.com/auroraguorui/Quantitative-Research/tree/master/CAT_IPD" target="_blank">code</a>]</p>
			  <p style="font-size:110%"><b>Method</b>: Hypothesis testing, Statistical Inference, Central limit theory</p>
			  <p style="font-size:110%"><b>Funded by</b>: <a href="https://www.collegeboard.org/" target="_blank">College Board</a></p>
			  <p>&nbsp;</p>
			  </ul>
			  </li>
              
			  <p>&nbsp;</p>
			  <li>
			  <a href="http://auroraguorui.github.io/projects.html#" class="big-link" data-reveal-id="ACT" data-animation="fade">
			  <img src="./Projects_files/act.png" width="215" height="180"></a>
			  <h3>ACT, Inc. Research Intern</h3>
			  <ul>
			  <p></p>
			  <p style="font-size:110%"><b>Project</b>: Multistage Adaptive Testing to Estimate Skills <a href="http://auroraguorui.github.io/file/Multistage_Testing_with_Item_Pool_Stratification.pdf" target="_blank">[paper]</a><a href="http://auroraguorui.github.io/file/2012ACT_Intern_RuiGuo.pdf" target="_blank">[slides]</a></p>
			  <p style="font-size:110%"><b>Method</b>: Fisher information, Logistic Regression, 
			  Heuristic methods, Greedy algorithm, Marginal maximum likelihood estimation, Expected a posterior estimation</p>
			  <p style="font-size:110%"><b>Funded by</b>: <a href="http://www.act.org/" target="_blank">ACT, Inc.</a></p>
			  <p>&nbsp;</p>
			  </ul>
			  </li>	
            
              <!--
			  <li>
			  <a href="#" class="big-link" data-reveal-id="ISBE" data-animation="fade" >
			  <img src="img/isbe.png"></a>
			  <h3>ISEBE Research Assistants</h3>
			  <ul>
			  <p></p>
			  <p style="font-size:110%"><b>Project</b>: Data Analysis for Illinois Standard Achievement Test</p>
			  <p style="font-size:110%"><b>Method</b>: Data integration, Data cleaning, 
			     Data augmentation, Chi-square testing, Fisher's exact test, Test fairness analysis, Bayesian DIF</p>
			  <p style="font-size:110%"><b>Funded by</b>: <a href="http://www.isbe.net/" target="_blank">Illinois state board of education</p> </a>
			  
			  <p></p>

			  
			  </li>

			  </ul>
			  </li>	
              -->
			  </ul></div>
			</div>
		  </div>
		</div>


		<!-- end #content -->
		<div style="clear: both;">&nbsp;</div>
		<p class="meta"></p>

		<p></p>
		<p>&nbsp;</p>
		<p></p>	
		
	  </div>
	  
	  <!-- end #page -->
	  <div id="footer">
		<p>Copyright (c) 2016 Rui Guo. All rights reserved. Design by <a href="http://www.freecsstemplates.org/">Free CSS Templates</a>. Last updated in Aug 2015</p>
	  </div>
	  <!-- end #footer -->
    </div>
  







<!------Begin model------------------------------------------------>
<!--------------------------------------------------------------->	
<div id="HSK" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Computer-based Testing System</h2>
      <p class="meta"></p>
	  <div class="entry">
	    <h3>Introduction</h3>
          <p>
              <a href="http://english.hanban.org/" target="_blank">Hanban</a> has developed 
              <a href="http://english.hanban.org/node_8002.htm" target="_blank">HSK</a> to help evaluate the 
              Chinese levels of non-native Chinese speakers across the world. HSK testing has 500,000 examinees 
              each year. Trainditional HSK is based on linear fixed form testing (i.e., each test taker receveis 
              the same form of testing) rather than <a href="https://en.wikipedia.org/wiki/Computerized_adaptive_testing" target="_blank">
              computerized adaptive testing (CAT)</a> (i.e., each test taker receives a individualized test form
              tailor for his/her demonstrated level of knownledge and skill). Given the fact that CAT has gaining 
              increasing popularity in educational testing, Hanban has decided to move from linear fixed form 
              testing to CAT. The project is funded by <a href="http://www.chinesetest.cn/linkusm.do" target="_blank">Chinese Testing International Co., Ltd</a>, and is separated into three stages: 
              </p><ol type="1">
              <li>Initial item analysis and item pool building</li> 
              <li>Computerized adaptive testing algorithm</li>
              <li>Web-based test development</li>
              </ol>
              <img src="./Projects_files/HSK0.png" width="700" height="400">
          <p></p>
          <p>&nbsp;</p>
    
      
          <p class="meta"></p>
          <p></p>
          <h3>Step 1: Initial Item Analysis</h3>
	      <p>
              Item analysis is a process which examines student responses to individual test items (questions) in 
              order to assess the quality of those items and of the test as a whole. Item analysis is especially 
              valuable in improving items which will be used again in later tests, but it can also be used to 
              eliminate ambiguous or misleading items in a single test administration. 
              I did item analysis of the HSK testing items using a plenty of statistical tools using both 
              <b>classical testing theory</b> (CTT) and <b>logistic regression</b>. Specifically, using CTT, 
              difficulty and discrimination distributions are computed for each item. Also, item reliability 
              coefficients are analyzed using <b>Cronbach's alpha</b> correlation, <b>Spearman Brown</b> correlation, and 
              <b>split-half</b> correlation, etc. Using logistic regression model, feature parameters are estimated 
              using <b>marginal maximum likelihood</b> implemented with <b>EM algorithm</b> for each test item. 
          </p>
    
          <p class="meta"></p>
          <p></p>
          <h3>Step 2: Computerized adaptive testing algorithm</h3>
          <p>
              I developed the computerized automatical test assembly program for HSK level 4. I wrote R source 
              codes to implement on-the-fly item selection algorithm. The results were presented at the 8th 
              Confucius Institute Conference at Beijing. The algorithm is described as follows:
          </p>
              <ol type="1">
                  <li>The pool of available items is searched for the optimal item, based on the current estimate 
                      of the examinee's ability using <b>maximum fisher information</b> criterion. 
                      algorithms to generate parallel tests forms from the item pool</li>
                  <li>The chosen item is presented to the examinee, who then answers it correctly or incorrectly
                  </li>
                  <li>The ability estimate is updated using logistic regression, based upon all prior 
                      answers</li>
                  <li>Steps 1-3 are repeated until a termination criterion, for example, a given <b>standard error 
                      of measurement</b>, is met</li>
              </ol>

    
          <p class="meta"></p>
          <p></p>
          <h3>Step 3: Web-based test development</h3>
          <p>
              I established a web-based HSK testing delivery system using <b>Python</b>, <b>Django</b> and <b>MySQL</b> to inplement
              the computerized adaptive version of HSK as described in step 2. A <a href="https://pacific-thicket-6533.herokuapp.com/Level4/register/" target="_blank">sample test platform </a>
              can be found here deployed with <b>Heroku</b>, and the <a href="https://github.com/auroraguorui/HSK_web_application" target="_blank">source code</a> is here. In the developed system, there are three main tables in the MySQL database:
              </p><ul>
                  <li type="square">Question table: all information of questions, including questino id, question text, answer key, difficulty parameter, discrimination parameter, guessing parameter and question type</li>
              </ul>
              <ul>
                  <li type="square">Answer table: tracking test takers' behaviror. When one question is finished, one entry is added to this table recording question id, student id, score, student's answer, current ability level estimate. After that, a maximum fisher information criterion is employed to select the most approriate item as the next question that can minimize the standard error of measurement of examinee ability</li>
              </ul>
              <ul>
                  <li type="square">Examinee table: all information of examinees, including whether an examinee has finished a subtest. If yes, a final ability estimate will be updated</li>
              </ul>
          <p></p>
          <p>
              <img src="./Projects_files/HSK.png" width="700" height="400">          
          </p>
          <p class="meta"></p>
	  </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>
	
<!--------------------------------------------------------------->	
<div id="indeed" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Indeed Web Crawling</h2>
      <p class="meta"></p>
	  <div class="entry">
	    <h3>Introduction</h3>
          <p>
              Being a data scientist requires a large skill set. To master all of that at a high level would probably take a lifetime! So which of these skills are most employers actually looking for? It is interesting to know the demand for data scientist for each state all over the country. It would also be nice to see if different cities have different skills they like to emphasize. Does the Silicon Valley market have different skills they prefer compared to New York City?
          </p>
      
          <p class="meta"></p>
          <p></p>
          <h3>Step 1: Web Crawling</h3>
	      <p>
              The first step is to use <b>Scrapy</b> to crawl data scientist job postings from <a href="http://www.indeed.com/" target="_blank">http://www.indeed.com</a>. This crawls through the whole website for data scientist jobs over pagination links at the bottom of each page and scrapes the postings (posting title and URL) from each page. 
              For each job posting, the <b>job title</b>, <b>company</b>, <b>city</b>, <b>state</b>, <b>url</b> and <b>descriptions</b> are extracted from the webpage. Job descriptions are <b>parsed</b>, <b>stripped</b>, <b>stemmed</b> and <b>lowercased</b> into words. The output data is pipelined to a <b>MongoDB</b> database.
          </p>
    
          <p class="meta"></p>
          <p></p>
          <h3>Step 2: Visualize Job Postings</h3>
          <p>
              I visualized crawled data to summarize number of job postings for each state, and compute key terms for a data scientist job in each state. These are the key data science skills we are looking for. Finally, I presented the results using a <a href="http://auroraguorui.github.io/indeed.html" target="_blank">website</a>. 
          </p>
          <p><img src="./Projects_files/indeed-web.png" width="680" height="380/"></p>
          <p>
              In the graph, dark color represents a high demand of data scientists. It looks like California has the highest job demand for data scientists, followed by New York, and Michigan. In each state, job seeker are looking a slightly different profile. However, the most import skills required include: R, Python, SQL, Hadoop, etc. The <a href="https://github.com/auroraguorui/Web-Scraping-and-API/tree/master/Indeed_Web_Scraping" target="_blank">source code</a> can be found in my Github repository.
          </p>
          

          
          <p class="meta"></p>
	  </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->		
<div id="Coupon" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Coupon Purchase Prediction</h2>
      <p class="meta"></p>
	  <div class="entry">
	    <h3>Introduction</h3>
	    <p>Ponpare is a Japanese leading coupon company that offers discount coupons for 
	    all kinds of goods and services. The goal of this project is to build a <b>recommender system</b> to customers
	    and predict which coupons each user would purchase in a period of time given each 
	    customer's past purchase and browsing behavior. 
	    <a href="https://www.kaggle.com/c/coupon-purchase-prediction" target="_blank">Here</a> 
	    is the original description on the Kaggle webpage.</p>
	    <p> We are provided with a year of transactional data for 22,873 users on the site 
	    <a href="http://ponpare.jp/" target="_blank">ponpare.jp</a>. Data includes a 
	    collection of detailed information of each customer, each coupon in the training 
	    and testing set, and browsing and purchase records during a 51-week time period from 
	    2011-07-01 to 2012-06-23. The whole <a href="https://www.kaggle.com/c/coupon-purchase-prediction/data" target="_blank">dataset</a> can be found here.</p>
	    <p> </p>
	    <p> </p>
        <p class="meta"></p>
	    <h3>Preprocessing</h3>
	    <p>We chose to use <b>content-based collaborative filtering</b> with a modified <b>cosine similarity</b>. 
	    In order to realize it, all categorical and ordinal features need to be transformed 
	    to a sequence of binary variable with 0 and 1 coding. 
	    A necessary step is to represent each coupon by a vector of its features. For example,
	    the feature named "Large Area" with 13 categories is transformed into 13 features of
	    with each representing one category, whose value is either 1 indicating this customer
	    belongs to this area and or 0 otherwise.</p>
	    <p>Also we did <b>feature scaling</b> on some of the numerical variables including
	    catalog price, discount rate, discounted price, etc. using <b>box-cox transformation</b>.</p>	    
	    <p> </p>
	    <p> </p>
        <p class="meta"></p>
	    <h3>Content Based Recommender System</h3>
	    <p>Since no review scores about coupons are available, we decided to use a content
	    based recommender system. Specifically, we quantified the similarity between any 
	    two coupons using a weighted version of cosine similarity, and assigned a preference
		score for each coupon in the test set to each customer. In order to find a proper 
		weight, we split data and used a cross validation method to find an optimal weight 
		combination for each feature.</p>
	    <p></p>
	    <p></p>
        <p class="meta"></p>
        <h3>Feature Engineering</h3>
	    <p>In order to improve the performance of the recommender system, we also did some
	    <b>feature engineering</b>. First we used data visualization tools to explore the whole 
	    dataset, trying to find some interesting patters. For example, the following plots
	    the age, and sex distributions of customers. </p>
	    <img src="./Projects_files/coupon_data.png" width="760" height="500">
	    <p>The detailed <a href="https://public.tableau.com/profile/rui.guo#!/vizhome/CouponPurchasePrediction/CouponPurchasePrediction" target="_blank">visualization</a> of the dataset can be found 
	    here. 
	    The data tells us that a different weighting structure between male and female, different
	    age group and different area is preferred. Then we modified our weight matrix based
	    on these findings by assigning different weights among people from different age groups, 
	    genders and areas. Also, we aggregated user's browsing history and purchase record
	    and aggregated 9 "USABLE_DATE" features to create a new feature. Finally, we computed
	    distance between coupon and user using the "LONGITUDE" and "LATITUDE" features from 
	    the original data.</p>
	    <p></p>
	    <p></p>
        <p class="meta"></p>
	    <h3>Results</h3>
	    <p>The result showed a score of 0.009417 using the <a href="https://www.kaggle.com/c/coupon-purchase-prediction/details/evaluation" target="_blank">
	    Mean Average Precision @ 10</a> evaluation metric, which ranked the <b>top 5%</b> among 1089 teams.
	    The <a href="https://github.com/auroraguorui/Kaggle_Competition/tree/master/coupon_purchase_prediction" target="_blank">source code</a> can be found here.</p>
	  </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="sentiment" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Sentiment Analysis on Movie Reviews</h2>
      <p class="meta"></p>
	  <div class="entry">
        <h3>Introduction</h3>
          <p>Maas et al (2011) constructed a collection of 50,000 reviews from IMDB movie reviews.
             This large movie review dataset contains a even number of positive and negative reviews
             with highly polarized reviews (i.e., neutral reviews are not included). The goal of this
             project is to classify positive and negative reviews using <b>natural language processing</b> techniques. 
          </p>
        <p class="meta"></p>
	    <h3>Preprocessing</h3>
          <p>
              The training data contains a labeled dataset and a unlabeled file without sentiment tags. The 
              labeled data file has three columns: "person ID", "sentiment" and "reviews", while the unlabeled 
              training data does not have the "sentiment" column. The "reviews" column has raw text reviews.
              Then I built a word cloud revealing the importance of each word for each category of sentiment. The 
              <b>BeautifulSoup</b> library was used to remove HTML tags, <b>regular expression</b> and nltk's punkt tokenizer 
              were used to remove numbers and punctuations. Finally, the <b>nltk</b> corpus was used to remove all 
              <b>stopwords</b>. Ater preprocessing, I built up a word cloud revealing the relative importance of each 
              word for each sentiment. The detailed <a href="https://public.tableau.com/profile/rui.guo#!/vizhome/SentimentAnalysisonMovieReviews/WordCloud-" target="_blank"> visualization</a> can be found here.
          </p>
        <p class="meta"></p>
        <h3>Bag of Words Model, Word2Vec and Doc2Vec Models</h3>
          <p>
              I chose bag of words model with 500,000 most frequent words as features, and vectorized them using 
              <b>TF-IDF</b>. Classification tools such as <b>logistic regression</b>, <b>stochastic gradient descent</b> and <b>adaboost</b> 
              etc. were employed. Then I trained the bag of words model using classification tools including 
              logistic regression, <b>naive bayes</b>, stochastic gradient descent and <b>classification tree</b> separately. 
              I also used <b>Word2Vec</b> and <b>Doc2Vec</b> models, which are <b>neural network</b> implementations that learns 
              distributed representation of words and documents, to learn unlabeled training data file. After 
              Word2Vec model and Doc2Vec model have been pretrained, 5000 features were generated using the 
              established vocabulary. The same classification tools were carried for the models too.
          </p>
          <p><img src="./Projects_files/sentiment_full.png" height="280" width="760"></p>
        <p class="meta"></p>
        <h3>Ensembling Results</h3>
          <p>
              Finally, I <b>ensembled</b> all classfiers into one. After tuning weigths, the best model generated a 
              resulting score of 97.044% using <a href="https://www.kaggle.com/c/word2vec-nlp-%20%20%20%20%20%20%20%20%20%20%20%20%20%20tutorial/details/evaluation" target="_blank">area under ROC curve</a>. The <a href="https://github.com/auroraguorui/Kaggle_Competition/tree/master/Bag_of_words">source code</a> can be found here.
          </p>
    </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="Digit" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Digit Recognizer</h2>
      <p class="meta"></p>
	  <div class="entry">
        <h3>Introduction</h3>
          <p>The goal in this project is to take an image of a handwritten single digit, and determine what that digit is.
	  The data for this project were taken from the MNIST dataset. The MNIST ("Modified National Institute of 
	  Standards and Technology") dataset is a classic within the Machine Learning community that has been extensively studied.  
	  More detail about the dataset, can be found <a href="http://yann.lecun.com/exdb/mnist/index.html" target="_blank">here</a>.
          </p>
	    <h3>Preprocessing</h3>
          <p>The training data contains a labeled dataset and a unlabeled file without sentiment tags. The labeled
            data file has three columns: "person ID", "sentiment" and "reviews", while the unlabeled training data 
            does not have the "sentiment" column. The "reviews" column has raw text reviews 
            Then I built a word cloud revealing the importance of each word for each category of sentiment. The BeautifulSoup
            library was used to remove HTML tags, regular expressions and nltk's punkt tokenizer were used to remove numbers and punctuations.
            Finally, the nltk corpus was used to remove all stopwords. Ater preprocessing, I built up a word cloud revealing the relative
            importance of each word for each sentiment. The detailed visualization can be found <a href="https://public.tableau.com/profile/rui.guo#!/vizhome/SentimentAnalysisonMovieReviews/WordCloud-" target="_blank"> here</a>
          </p>
        <h3>Bag of Words Model, Word2Vec and Doc2Vec Models</h3>
          <p>I compared different <b>machine learning algorithms</b> including support vector machines, 
	  logistic regression with <b>regularization</b>, naive bayes and random forests using 10-fold <b>cross validation</b> to
	  select the best algorithm and tuning parameters.
          </p>
          <p><img src="./Projects_files/sentiment_full.png" height="280" width="760"></p>
        <h3>Results</h3>
          <p>The left figure is the example. 
	  The right figure is the comparison of different algorithms. 
	  The best classification algorithm for this case is the random forests
	  algorithm. For the validation set, I got 99.7% accuracy, and for the test set, I got 96.3% accuracy.</p>
	  <img src="./Projects_files/digit.png" height="420" width="500"><p></p>
    </div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="Titanic" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Survival Prediction</h2>
	<p class="meta"></p>
	<div class="entry">
	  <h3>Introduction</h3><p></p>
	  <p style="font-size:110%">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, 
	  the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked 
	  the international community and led to better safety regulations for ships.One of the reasons that the shipwreck led to such loss 
	  of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in 
	  surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.
	  In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to 
	  apply the tools of machine learning to predict which passengers survived the tragedy.</p>
	  <p style="font-size:110%">For each passenger in the test set, I predicted whether or not they survived the sinking (0 for deceased, 1 for survived). </p>

	  <h3>Methods</h3><p></p>
	  <p style="font-size:110%">I compared different <b>machine learning algorithms</b> including support vector machines, 
	  logistic regression with <b>regularization</b>, naive bayes and random forests using 10-fold <b>cross validation</b> to
	  select the best algorithm and tuning parameters. The best classification algorithm for this case is the random forests
	  algorithm. For the validation set, I got 83.61% accuracy, and for the test set, I got 80.38% accuracy.</p>
	  
	  <h3>Results</h3><p>&nbsp;</p>
	  <img src="./Projects_files/titanic_figure.png" height="420" width="760">
	  <br><br>
	</div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="SF" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Predict the category of crimes by the bay</h2>
	<p class="meta"></p>
	<div class="entry">
	  <h3>Introduction</h3><p></p>
	  <p>From 1934 to 1963, San Francisco was infamous for housing some of the world's most notorious criminals on the inescapable island of Alcatraz.
	  Today, the city is known more for its tech scene than its criminal past. But, with rising wealth inequality, housing shortages, and a proliferation of expensive digital toys riding BART to work, there is no scarcity of crime in the city by the bay.
	  From Sunset to SOMA, and Marina to Excelsior, this competition's dataset provides nearly 12 years of crime reports from across all of San Francisco's neighborhoods. Given time and location, you must predict the category of crime that occurred.
      </p>
      <p>This dataset is brought to you by SF OpenData, the central clearinghouse for data published by the City and County of San Francisco. This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set. 
      </p>
      <h3>Data</h3>
      <p>This dataset contains incidents derived from SFPD Crime Incident Reporting system. The data ranges from 1/1/2003 to 5/13/2015. The
          training set and test set rotate every week, meaning week 1,3,5,7... belong to test set, week 2,4,6,8 belong to training set.</p>
      <p>Dates - timestamp of the crime incident. Category - category of the crime incident (only in train.csv). This is the target variable you
          are going to predict. Descript - detailed description of the crime incident (only in train.csv). DayOfWeek - the day of the week. 
          PdDistrict - name of the Police Department District. Resolution - how the crime incident was resolved (only in train.csv). Address - the
          approximate street address of the crime incident. X - Longitude. Y - Latitude</p>
	  <h3>Methods</h3>
	  <p>I compared different <b>machine learning algorithms</b> including support vector machines, 
	  logistic regression with <b>regularization</b>, naive bayes and random forests using 10-fold <b>cross validation</b> to
	  select the best algorithm and tuning parameters. The best classification algorithm for this case is the random forests
	  algorithm. For the validation set, I got 83.61% accuracy, and for the test set, I got 80.38% accuracy.</p>
	  <p>The data visualization can be found <a href="https://public.tableau.com/profile/rui.guo#!/vizhome/SanFranciscoCrimeClassification_0/Story2"> here</a>.
	  </p><h3>Results</h3>
      <p>&nbsp;</p>
	  <br><br>
	</div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="CB" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">ACCUPLACER<sup>®</sup> Data Analysis</h2>
	<p class="meta"></p>
	<div class="entry">
	  <h3>Introduction</h3>
      <p>
          This project is supported by <a href="https://www.collegeboard.org/" target="_blank">College Board</a>
          as student research fellowship with full tuition and stipend. ACCUPLACER<sup>®</sup> is an 
          integrated system of computer-adaptive assessments designed by College Board to evaluate students’ 
          skills in reading, writing, and mathematics. Delivering results immediately and precisely, ACCUPLACER 
          offers diagnostics and intervention support to help students prepare for academic course work. Each 
          year, ACCUPLACER<sup>®</sup> connects more than 2.5 million students to college and career 
          opportunities. 
      </p>
      
      <p class="meta"></p>
	  <h3>Methods</h3>
      <p>
          College Board wants to know whether <b>test bias</b> exists between different groups of people such as male 
          and female, white and black, etc. The data is highly <b>sparsed</b> with a half <b>systematic missing values</b>. I solved the estimation inefficiency problem of data with systematic missing 
          entries by implementing a bayesian <b>expectation-maximization</b> (EM) algorithm using Matlab. 
      </p>
      <p>
          I invented a <b>non-parametric statistical hypothesis testing</b>, namely, 
          a modified SIBTEST, derived from <b>central limit theory</b> to detect item parameter drift (IPD) and
          differential item functioning (DIF), which evaluates differences in item performance for members from reference (R) and focal (F) groups. To operationalize SIBTEST, the statistical <b>hypothesis</b> of interest is
      </p>
      <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<b>H<sub>o</sub></b>: No difference between focal and reference group; &nbsp;<b>H<sub>a</sub></b>: Difference between two groups</p>
      <p>
          The <b>test statistic</b> is 
      </p>
      <p>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="./Projects_files/cb_t.png" width="75" height="30/">
      </p>
        where 
      <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="./Projects_files/cb_beta.png" width="150" height="25/">
      </p>and
      <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="./Projects_files/cb_variance.png" width="300" height="35/">
      </p>
      <p>
          According to central limit theory, the test statistic follows a standard normal distribution. The null hypothesis is rejected if T is greater than Z<sub>α</sub>.
      </p>
	  
      <p class="meta"></p>
	  <h3>Results</h3>
	  <p>
          The simulation study showed a high classification accuracy using the proposed hypothesis testing in terms of <b>false positive</b> and <b>false negative rates</b> and an
          improved estimation efficiency using EM algorithm compared with existing methods, for example, maximum likelihood estimation. The <a href="http://auroraguorui.github.io/file/College_board.pdf" target="_blank">research report</a> can be found here and a sample <a href="https://github.com/auroraguorui/Quantitative-Research/tree/master/CAT_IPD" target="_blank">source code</a> can be found here.
      </p>
	</div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="ACT" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Multistage Adaptive Testing for Skill Estimation</h2>
	<p class="meta"></p>
	<div class="entry">
	  <h3>Introduction</h3>
      <p>
          ACT WorkKeys<sup>®</sup> is a job skills assessment system that helps employers select, hire, 
          train, develop, and retain a high-performance workforce. This series of tests measures foundational and 
          soft skills and offers specialized assessments to target institutional needs. As part of ACT's Work 
          Readiness System, ACT WorkKeys<sup>®</sup> has helped millions of people in high schools, colleges, 
          professional associations, businesses, and government agencies build their skills to increase global 
          competitiveness and develop successful career pathways. One challenge of WorkKeys is to classify test 
          takers' skills into levels from high to low accurately and rapidly. During my internship in ACT, I designed an algorithm to improve classification efficiency and effectiveness. </p>
        
      <p class="meta"></p>
	  <h3>Methods</h3>
      <p>
          I developed an innovative <b>computer-based adaptive test delivery</b> mode using logistic regression for 
          <b>skill classification</b> and <b>information theory</b> for <b>on-the-fly adaptive</b> design. The idea is to divide the 
          new design into two stages in which the former stage has a medium test difficulty whereas the latter 
          stage has a tailored difficulty level based on skill classification results in the first stage. In 
          detail, in the first stage, a group of test items were randomly chosen and an initial skill 
          classification is obtained, while in the second stage, a target fisher informaiton distribution named 
          "pathway" is computed based on the initial classification results, and test items are chosen based on 
          their closeness to the target information curve. Specifically, in the second stage, I developed a <b>heuristic bottom-up greedy algorithm</b> called normalized weighted absolute deviation heuristic (NWADH) to assemble 
          the panel test on-the-fly so that each test has similar <b>fisher information curve</b>.</p>
      <img src="./Projects_files/ACT_1.png" height="300" width="380">
	  <img src="./Projects_files/ACT_4.png" height="300" width="380">
	  <p>&nbsp;</p>
        
      <p class="meta"></p>
	  <h3>Results</h3>
      <p>
          After implementing my design, classification efficiency has been reduced by 40% from simulation study 
          and the cost of item development can be saved by 10%. The new design also has other advantages such as 
          the improvement of test security. The source code has been adopted by ACT, Inc. for further commericial development and the research 
          <a href="http://auroraguorui.github.io/file/Multistage_Testing_with_Item_Pool_Stratification.pdf" target="_blank">paper</a> has been published at the <a href="http://www.ncme.org/NCME" target="_blank">National Council on Measurement in Education</a>. The source code is confidential.
      </p>
	  <img src="./Projects_files/ACT_2.png" height="300" width="380">
	  <img src="./Projects_files/ACT_3.png" height="300" width="380">

	</div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="ISBE" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">ISBE</h2>
	<p class="meta"></p>
	<div class="entry">
	  <h3>Introduction</h3>
	  <p>
          Analyzed Illinois Standard Achievement Test dataset in SAS  by longitudinal study using multilevel 
          modeling with time series. Reinforced the test fairness by performing EFA and CFA factor analysis, 
          logistic regression, and ANOVA.
      </p>
        
      <p class="meta"></p>
	  <h3>Methods</h3>
	  <p>
          I compared different <b>machine learning algorithms</b> including support vector machines, 
          logistic regression with <b>regularization</b>, naive bayes and random forests using 10-fold <b>cross 
          validation</b> to select the best algorithm and tuning parameters. The best classification algorithm 
          for this case is the random forests algorithm. For the validation set, I got 99.7% accuracy, and for 
          the test set, I got 80.383% accuracy.
      </p>
	</div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>

<!-------------------------------------------------------->	
<div id="UI" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Mining student response data for knowledge using Q-matrix method</h2>
	<p class="meta"></p>
	<div class="entry">
	  <h3>Introduction</h3>
	  <p>
          A "smart testing" is a specific mathematics assessment that reveals thinking which provide teachers 
          with an informative diagnosis of their students' conceptual understanding of most of the topics of 
          knowledge. Smart tests are currently being used in schools and further tests are being developed.
      </p>
      <p>
          Although many talented researchers have created excellent tools for computer-assisted instruction and 
          intelligent tutoring systems, creating high-quality, effective, scalable but individualized tools for 
          learning at a low cost is still an open research challenge. In this project, we used a q-matrix method, 
          where data from student behavior is "mined" to create concept models of the material being taught. 
          These models are then used to both understand student behavior and direct learning paths for future 
          students. 
	  </p>
	  <p>          
          The objective is to develop an online assessment system to provide cognitive diagnostic assessment 
          feedback to the students to help them meet the 
          standards and assist teachers to more efficiently teach. 
      </p>
        
      <p class="meta"></p>
	  <h3>Q-matrix algorithm</h3>
	  <p>
          The idea of determining a student's knowledge state from her test question responses inspired the 
          creation of a Q-matrix, a binary matrix showing the relationship between test items and latent or 
          underlying attributes, or concepts from student response data. Students were assigned knowledge states 
          based on their test answers and the constructed Q-matrix. A Q-matrix contains a one if a question is 
          related to the concept, and a zero if not. For example, in the following left, questions 1 requires a 
          mastery of matrix addition, multiplication, division and sigular value decomposition; item 2 requires 
          matrix addition, division, and determinant computation; while item 3 requires matrix addition, 
          subtraction, multiplication and sigular value decomposition. In the follwing right, Bo has mastered the last three skills and Jane has mastered the 2nd, 4th and 5th skills, while they both have the same total score. With this information, students are provided with diagnostic information and teachers can make instructional decisions.
      </p>
      <p class="meta"></p>
          <img src="./Projects_files/Q-matrix_1.png" height="130"> 
          <img src="./Projects_files/Q-matrix.png" height="130"> 
      <p class="meta"></p>
      <h3>DINA Model</h3>
      <p>
          DINA model stands for "Deterministic Input, Noisy And Gate" model, has the following mathematical structure
      </p>
      <p align="right">
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="./Projects_files/cd-model.png" width="200" heigh="25/">
          
      </p>
      <p>
          where 
          </p><ul>
              <li>
                  s<sub>j</sub> and g<sub>j</sub> are the slipping and guessing parameters of item j;
              </li>
              <li>
                  η<sub>ij</sub> =  ∑<sub>k</sub> α<sub>ik</sub><sup>jk</sup> is a latent response variable connecting the Q matrix and attributes;
              </li>
              <li>
                  η=1 indicates that the i<sup>th</sup> examinee have possessed all the required attributes of item j .
              </li>
          </ul>
      <p></p>
      <p>
          The corresponding information matrix for the slipping and guessing parameter for a single item j with examinee i is
      </p>
      <p>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="./Projects_files/CD-information.png" width="400" height="50/">
      </p>
      <p>Maxmizing the determinant value of the above information matrix corresponds to a D-optimal design to calibrate model parameters</p>
      <h3>A Shannon Entropy Algorithm to select items</h3>
      <p>
          To implement the Q-matrix algorithm, we used a <b>linear conjunctive latent class model</b> together with
          <b>Shannon entropy</b>(SHE) method to classify whether a student has mastered a particular concept. 
          The Shannon entropy algorithm tries to minimize the expected Shannon entropy of the <b>posterior distribution</b> of <b>α</b>, which can be written as
      </p>
      <p>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="./Projects_files/cd-posterior.png" width="200" height="50/">
      </p>
      Then the Shannon entropy of the posterior distribution g<sub>t</sub> can be written as:
      <p>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <img src="./Projects_files/cd-shannon.png" width="150" height="40/">
      </p>
      <p class="meta"></p>
	  <h3>Results</h3>
        <p>
            Result are evaluated in terms of estimation bias and root mean squared error. Since the proposed method can help pinpoint students' mastery level on each skill, thus can  enable students focus on studying topics not mastered. A practial field application have been found in Physics 101 course in University of Illinois, resulting in a 10% decrease in course drop rate.
            A sample <a href="https://github.com/auroraguorui/Quantitative-Research/tree/master/CDCAT_ONLINE" target="_blank">source code</a> of this project can be found here.
        </p>
        
	</div>
  </div>
  <a class="close-reveal-modal">×</a>
</div>



<!-------------------------------------------------------->	
<div id="Springleaf" class="reveal-modal xlarge" data-reveal="">
  <div class="post">
    <h2 class="detail-title" align="center">Determine whether to send a direct mail piece to a customer</h2>
	<p class="meta"></p>
	<div class="entry">
	  <h3>Introduction</h3>
	  <p>
          Springleaf puts the humanity back into lending by offering their customers personal and auto loans that help them take control of their lives and their finances. Direct mail is one important way Springleaf's team can connect with customers whom may be in need of a loan.
          Direct offers provide huge value to customers who need them, and are a fundamental part of Springleaf's marketing strategy. In order to improve their targeted efforts, Springleaf must be sure they are focusing on the customers who are likely to respond and be good candidates for their services.
	  </p>
	  <p>          
          Using a large set of anonymized features, Springleaf is asking you to predict which customers will respond to a direct mail offer. You are challenged to construct new meta-variables and employ feature-selection methods to approach this dauntingly wide dataset.
      </p>
        
      <p class="meta"></p>
	  <h3>Data</h3>
	  <p>
          We are provided a high-dimensional dataset of anonymized customer information. Each row corresponds to one customer. The response variable is binary and labeled "target". We must predict the target variable for every row in the test set.
      </p>
      <p>
          The features have been anonymized to protect privacy and are comprised of a mix of continuous and categorical features. The meaning of the features, their values, and their types are provided "as-is" for this competition; handling a huge number of messy features is part of the challenge here.
      </p>
        
      <p class="meta"></p>
      <h3>Dig into Data using R</h3>
      <p>
          First since the data is high-dimensional and cannot fit into memory of a single machine, we just sampled the first 1000 observation to get an overall sense of the data using R. Later, Apache-Spark will be used to take fully usage of the whole dataset. 
      </p>
      <p>
          Then we checked missing value, duplicates and unique value. We also did feature engineering on time variables by extracting day of week, week of year, month of year, and etc. Next, for geographical variables, we ploted values on a map grouped by states. For numerical values, we drilled down to the empirical distribution and Pearson correlations of them. Please see detailed <a href="https://github.com/auroraguorui/Kaggle_Competition/blob/master/Springleaf/Springleaf_R.ipynb" target="_blank">R code</a> here.      
      </p>
      <p>
          <img src="./Projects_files/map.png" height="450" width="750/">
      </p>
      <p>
          <img src="./Projects_files/correlation.png" height="550" width="750/">
      </p>
      <p>
          Maxmizing the determinant value of the above information matrix corresponds to a D-optimal design to calibrate model parameters
      </p>
      <h3>Classification using Spark</h3>
      <p>
          In this step, we used Apache-Spark to perform the classfication job given that the data cannot fit into memory of local machines. First, data is uploaded and stored into Amazon Web Service. Then we pulled data from AWS to databricks. We used Spark SQLContext to create dataframe, and used Spark mllib package to build the classifier. The classification accuracy is 80% for the test set. Please see detailed <a href="https://github.com/auroraguorui/Kaggle_Competition/blob/master/Springleaf/Springleaf_Spark.ipynb" target="_blank">Spark source code</a> here.
      </p>
  </div>
  <a class="close-reveal-modal">×</a>
</div></div></body></html>